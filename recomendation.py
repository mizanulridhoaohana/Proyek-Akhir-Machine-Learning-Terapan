# -*- coding: utf-8 -*-
"""recomendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4qQWlNg0aibV5pABNsMpzyWCXsN-AKk

# Proyek Akhir Machine Learning Terapan DIcoding

## Import Library & Dataset
"""

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
import datetime

song_df = pd.read_csv('data.csv')
song_df = pd.DataFrame(song_df)
song_df

"""## Exploratory Data Analytics"""

# menghapus data pada column Unnamed
song_df = song_df.drop(['Unnamed: 0'], axis=1)
song_df.info()

# cek kekosongan data
song_df.isnull().sum()

# Cek duplikasi data
song_df.duplicated().sum()

# Membuat variabel baru age
yr = datetime.datetime.now().year

song_df['year'] = song_df['release_date'].str.extract(r'(\d{4})')

# Pastikan kolom "year" dalam bentuk integer
song_df['year'] = song_df['year'].astype(int)

song_df['age'] = yr - song_df['year']

song_df

# drop kolom yang mengandung stringa dan tampilkan korelasi
df_2 = song_df.drop(['id','name','album','artist','release_date'], axis=1)
corr = df_2.corr()
corr.style.background_gradient(cmap='coolwarm')

"""Berdasarkan visualisasi tersebut, fitur yang memiliki korelasi tertinggi adalah Energy dan loudness."""

numeric_cols = ['duration_ms',	'popularity',	'danceability',	'acousticness',	'danceability.1',	'energy',	'instrumentalness',	'liveness',	'loudness',	'speechiness',	'tempo',	'year',	'age']
sns.pairplot(df_2[numeric_cols])

# cek popularitas sebagai nilai target yang ingin dijadikan sebagai acuan
sns.distplot(song_df['popularity']).set_title('Popularity Distribution')

# Berdasarkan grafik di atas,
# bisa dilihat bahwa perubahan data terjadi ketika popularitas lebih dari 50

song_df.loc[song_df['popularity'] < 50, 'popularity'] = 0
song_df.loc[song_df['popularity'] >= 50, 'popularity'] = 1
song_df.loc[song_df['popularity'] == 1]

# Menghilangkan karakter spesial menggunakan regex

song_df['name'] = song_df['name'].str.replace(r'[.,()-]', '', regex=True)
song_df['album'] = song_df['album'].str.replace(r'[.,()-]', '', regex=True)


song_df

# cek duplikasi data menggunakan column name untuk mengantisipasi duplikasi data
boolean = song_df['name'].duplicated().any()
boolean

# ubah ke dalam pandas dataframe
data = pd.DataFrame(song_df,
                columns = ['name',	'album', 'artist', 'duration_ms',	'popularity', 'danceability', 'acousticness','danceability.1', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'year', 'age'])
duplicate = data[data.duplicated()]
duplicate

# droping duplicate values
data.drop_duplicates('name', inplace = True)

# cek duplikasi data setelah diproses
boolean = data['name'].duplicated().any()
boolean

data

# cek nilai unique yang ada di kolom poularity untuk memastikan tidak ada kesalahan data
data['popularity'].unique()

# visualisasi 15 artis dengan popularitas tertinggi

df=data
fig, ax = plt.subplots(figsize = (12, 10))
lead_artists = df.groupby('artist')['popularity'].sum().sort_values(ascending=False).head(15)
ax = sns.barplot(x=lead_artists.values, y=lead_artists.index, palette="Blues", orient="h", edgecolor='black', ax=ax)
ax.set_xlabel('Sum of Popularity', fontsize=12)
ax.set_ylabel('Artist', fontsize=12)
ax.set_title('15 Most Popular Artists in Dataset', fontsize=14, weight = 'bold')
plt.show()

df['mean'] = df.groupby('artist')['popularity'].transform('mean')
df['count'] = df.groupby('artist')['popularity'].transform('count')
# plotting
fig, ax = plt.subplots(figsize = (15, 5))
ax = sns.distplot(df['count'], bins = 300)
ax.set_xlabel('Count of apperances in data', fontsize=12, color='r')
ax.set_ylabel('% of artists', fontsize=12, color='r')
plt.show()

from yellowbrick.target import FeatureCorrelation

feature_names = ['danceability', 'acousticness', 'danceability.1',
                 'energy', 'instrumentalness', 'liveness', 'loudness',
                 'speechiness', 'tempo', 'year', 'age']

X, y = data[feature_names], data['popularity']

# Create a list of the feature names
features = np.array(feature_names)

# Instantiate the visualizer
visualizer = FeatureCorrelation(labels=features)

plt.rcParams['figure.figsize']=(10,5)
visualizer.fit(X, y)     # Fit the data to the visualizer
visualizer.show()

data

"""## Model Building"""

# using cosine similarity to calculate

data_new = data.drop(['artist','name','album'], axis=1)
train, test = train_test_split(data_new, test_size=0.3, random_state=42)

test = test.drop(['popularity'], axis=1).to_numpy()

liked_songs = train.loc[train['popularity'] == 1].drop(['popularity'], axis=1)
disliked_songs = train.loc[train['popularity'] == 0].drop(['popularity'], axis=1)

liked_songs_id = liked_songs.index
disliked_songs_id = disliked_songs.index

def normalize_2(arr):
    cols_mean, cols_stdev = arr.mean(axis=0), arr.std(axis=0)
    output = (arr - cols_mean) / cols_stdev
    return output

def get_track_features(idx_arr):
    track_features = []
    for idx in idx_arr:
        if idx in liked_songs_id:
            track = normalize_2(liked_songs.loc[idx].to_numpy())
        elif idx in disliked_songs_id:
            track = normalize_2(disliked_songs.loc[idx].to_numpy())

        if len(track) > 0:
            track_features.append(track)
    return np.array(track_features)

user_favorites = get_track_features(liked_songs_id)
user_dislikes = get_track_features(disliked_songs_id)

print('Jumlah lagu populer: {0}\nJumlah lagu tidak populer: {1}\n'.format(len(user_favorites), len(user_dislikes)))

from numpy import linalg as LA

fav_mean = user_favorites.mean(axis=0)

def top_5(tracks):
    cosine_similarities = [np.dot(fav_mean, sample)/(LA.norm(fav_mean)*LA.norm(sample)) for sample in tracks]
    zipped_similarities = zip(cosine_similarities, enumerate(tracks))
    sorted_similarities = sorted(zipped_similarities, reverse=True, key = lambda x: x[0])

    return [(idx,_similarity) for _similarity,(idx,_features) in sorted_similarities[0:5]]

# tampilkan 5 data teratas dari lagu

top_5_songs = top_5(test)

c=1
print('5 rekomendasi lagu baru teratas berdasarkan kemiripan dengan lagu populer : \n')
for song in top_5_songs:
    id = song[0]
    similarity = song[1]
    print('{0}. {1}'.format(c, data.loc[data.index[id]]['name']), similarity, "\n")
    c = c+1

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

X_NB = data.drop(['popularity','artist','name', 'album', 'count'], axis=1).to_numpy()
y_NB = data['popularity'].copy()

X_train_NB, X_test_NB, y_train_NB, y_test_NB = train_test_split(X_NB, y_NB, test_size=0.2, random_state=42)

#normalize
scaler = StandardScaler()
scaler.fit(X_train_NB)

X_train_NB = scaler.transform(X_train_NB)
X_test_NB = scaler.transform(X_test_NB)

features = ['duration_ms', 'danceability', 'acousticness', 'danceability.1',
                 'energy', 'instrumentalness', 'liveness', 'loudness',
                 'speechiness', 'tempo', 'year', 'age']
X_train = data[features]
y_train = data['popularity']

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 420)

sgd = SGDClassifier()
param_grid = {"alpha": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}
cv = GridSearchCV(sgd, param_grid, n_jobs=-1,verbose =1)
result = cv.fit(X_train_NB, y_train_NB)

print('\noptimal learning rate = {}'.format(result.best_params_))

model = result.best_estimator_


y_pred_sgd = model.predict(X_test_NB)
sgd_acc = accuracy_score(y_test_NB, y_pred_sgd)
print('\nsgd accuracy: {}'.format(sgd_acc))

from sklearn.tree import DecisionTreeClassifier

#decision tree
DT_Model = DecisionTreeClassifier()
DT_Model.fit(X_train, y_train)
DT_Predict = DT_Model.predict(X_test)
DT_Accuracy = accuracy_score(y_test, DT_Predict)

print("Accuracy: " + str(DT_Accuracy))

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=52)

# mendefinisikan parameter grid to search
param_grid = {
    'max_depth': [None, 3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [1.0, 'sqrt', 'log2']
}

# membuat model dengan Decision Tree
model = DecisionTreeClassifier()

# mengaplikasikan grid search untuk melakukan hypertuning parameter
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# menyimpan model terbaik dan parameternya
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

# membuat prediksi menggunakan model terbaik yang diperoleh
y_pred = best_model.predict(X_test)
y_pred

# menghitung nilai accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Best Parameters: {best_params}")
print(f"Accuracy: {acc}")